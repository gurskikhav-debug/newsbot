import os
import json
import requests
from datetime import datetime
from deep_translator import GoogleTranslator
import feedparser

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
TOKEN = os.getenv("TOKEN")
NEWSAPI_KEY = os.getenv("NEWSAPI_KEY")
ADMIN_ID = os.getenv("ADMIN_ID")
BOT_COMMAND = os.getenv("BOT_COMMAND", "/start")
KEYWORDS_INPUT = os.getenv("KEYWORDS", "")

# --- –ö–µ—à ---
CACHE_FILE = "cache/news_cache.json"

def load_cache():
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                return set(data) if isinstance(data, list) else set()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–µ—à–∞: {e}")
    return set()

def save_cache(cache_set):
    os.makedirs("cache", exist_ok=True)
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(list(cache_set), f, ensure_ascii=False, indent=2)

# --- –ü–µ—Ä–µ–≤–æ–¥ ---
def translate_text(text):
    try:
        return GoogleTranslator(source='auto', target='ru').translate(text)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
        return text

# --- –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π ---
def search_news(keywords):
    articles = []

    # 1. NewsAPI
    if NEWSAPI_KEY and keywords:
        try:
            url = "https://newsapi.org/v2/everything"
            params = {
                'q': ' OR '.join(keywords),
                'language': 'en',
                'sortBy': 'publishedAt',
                'pageSize': 20,
                'apiKey': NEWSAPI_KEY
            }
            r = requests.get(url, params=params, timeout=15)
            if r.status_code == 200:
                data = r.json()
                for item in data.get('articles', []):
                    articles.append({
                        'title': item['title'],
                        'url': item['url'],
                        'source': item['source']['name'],
                        'published': item.get('publishedAt', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                    })
            else:
                print(f"NewsAPI error {r.status_code}: {r.text}")
        except Exception as e:
            print(f"NewsAPI –æ—à–∏–±–∫–∞: {e}")

    # 2. RSS –∏–∑ –ö–∏—Ç–∞—è
    try:
        feeds = {
            'xinhua': 'http://www.xinhuanet.com/rss/world.xml',
            'sina': 'https://rss.sina.com.cn/news/china.xml',
            'sohu': 'http://rss.news.sohu.com/rss2/news.xml'
        }
        for name, feed_url in feeds.items():
            try:
                feed = feedparser.parse(feed_url)
                for entry in feed.entries:
                    title = entry.title.lower()
                    if any(kw.lower() in title for kw in keywords):
                        articles.append({
                            'title': entry.title,
                            'url': entry.link,
                            'source': name,
                            'published': entry.get('published', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                        })
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ RSS {name}: {e}")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS: {e}")

    # 3. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å–∞–π—Ç—ã
    try:
        tech_feeds = {
            'habr': 'https://habr.com/ru/rss/technology/',
            'techcrunch': 'https://techcrunch.com/feed/',
            'wired': 'https://www.wired.com/feed/rss'
        }
        for name, feed_url in tech_feeds.items():
            try:
                feed = feedparser.parse(feed_url)
                for entry in feed.entries:
                    title = entry.title.lower()
                    if any(kw.lower() in title for kw in keywords):
                        articles.append({
                            'title': entry.title,
                            'url': entry.link,
                            'source': name,
                            'published': entry.get('published', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                        })
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ RSS {name}: {e}")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö RSS: {e}")

    return articles

# --- –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Telegram ---
def send_message(chat_id, text, parse_mode='Markdown', disable_preview=False):
    if not chat_id:
        print("‚ùå chat_id –Ω–µ –∑–∞–¥–∞–Ω")
        return
    try:
        url = f"https://api.telegram.org/bot{TOKEN}/sendMessage"
        data = {
            "chat_id": chat_id,
            "text": text,
            "parse_mode": parse_mode,
            "disable_web_page_preview": not disable_preview
        }
        response = requests.post(url, data=data, timeout=10)
        if response.status_code == 200:
            print(f"‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ {chat_id}")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {response.status_code}, {response.text}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ: {e}")

# --- –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ---
def main():
    print("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω (—Ä–µ–∂–∏–º GitHub Actions)")
    seen_urls = load_cache()

    if BOT_COMMAND == "/start":
        msg = (
            "–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π.\n"
            "–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\n"
            "üîç /search ‚Äî –Ω–∞–π—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º\n"
            "üìã /help ‚Äî –ø–æ–º–æ—â—å"
        )
        if ADMIN_ID:
            send_message(ADMIN_ID, msg)

    elif BOT_COMMAND == "/search":
        if not KEYWORDS_INPUT.strip():
            if ADMIN_ID:
                send_message(ADMIN_ID, "‚ùå –ù–µ —É–∫–∞–∑–∞–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞.")
            return

        keywords = [kw.strip().lower() for kw in KEYWORDS_INPUT.split(',') if kw.strip()]
        print(f"üîç –ü–æ–∏—Å–∫ –ø–æ: {keywords}")

        raw_articles = search_news(keywords)
        print(f"–ü–æ–ª—É—á–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(raw_articles)}")

        if not raw_articles:
            if ADMIN_ID:
                send_message(ADMIN_ID, "‚ùå –ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
            return

        articles = [a for a in raw_articles if a.get('url') not in seen_urls]
        if not articles:
            if ADMIN_ID:
                send_message(ADMIN_ID, "üì≠ –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –≤–∞—à–∏–º —Å–ª–æ–≤–∞–º –Ω–µ—Ç.")
            return

        msg = f"üì¨ *–ù–æ–≤–æ—Å—Ç–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É:* `{', '.join(keywords)}`\n\n"
        for art in articles[:10]:
            title_ru = translate_text(art['title'])
            source = art.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            msg += f"üìå *{title_ru}*\nüåê {source}\nüîó {art['url']}\n\n"

        if ADMIN_ID:
            send_message(ADMIN_ID, msg, disable_preview=False)

        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–µ—à
        for art in articles:
            url = art.get('url')
            if url:
                seen_urls.add(url)
        save_cache(seen_urls)

    elif BOT_COMMAND == "/help":
        help_msg = (
            "üìå *–ü–æ–º–æ—â—å*\n\n"
            "–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\n"
            "üî∏ /start ‚Äî –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é\n"
            "üî∏ /search ‚Äî –Ω–∞–π—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏\n"
            "üî∏ /help ‚Äî —ç—Ç–∞ —Å–ø—Ä–∞–≤–∫–∞\n\n"
            "–ß—Ç–æ–±—ã –∏—Å–∫–∞—Ç—å, –∑–∞–ø—É—Å—Ç–∏—Ç–µ workflow –∏ —É–∫–∞–∂–∏—Ç–µ:\n"
            "  - command: `/search`\n"
            "  - keywords: `—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞, AI, 3D –ø–µ—á–∞—Ç—å`"
        )
        if ADMIN_ID:
            send_message(ADMIN_ID, help_msg, parse_mode='Markdown')

    else:
        if ADMIN_ID:
            send_message(ADMIN_ID, f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: {BOT_COMMAND}")

    print("‚úÖ –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

# --- –ó–∞–ø—É—Å–∫ ---
if __name__ == "__main__":
    main()
