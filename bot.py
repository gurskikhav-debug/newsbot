import os
import json
import requests
from datetime import datetime
from googletrans import Translator
import feedparser
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CommandHandler, CallbackQueryHandler, MessageHandler, filters, ContextTypes, ConversationHandler

# --- –°–æ—Å—Ç–æ—è–Ω–∏—è ---
AWAITING_KEYWORDS = "awaiting_keywords"

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
TOKEN = os.getenv("TOKEN")
NEWSAPI_KEY = os.getenv("NEWSAPI_KEY")
ADMIN_ID = os.getenv("ADMIN_ID")

CACHE_FILE = "cache/news_cache.json"
translator = Translator()

# --- –ö–µ—à ---
def load_cache():
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                return set(json.load(f))
        except:
            return set()
    return set()

def save_cache(cache_set):
    os.makedirs("cache", exist_ok=True)
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(list(cache_set), f, ensure_ascii=False, indent=2)

# --- –ü–µ—Ä–µ–≤–æ–¥ ---
def translate_text(text):
    try:
        return translator.translate(text, dest='ru').text
    except:
        return text

# --- –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π ---
def search_news(keywords):
    articles = []

    # 1. NewsAPI
    try:
        url = "https://newsapi.org/v2/everything"
        params = {
            'q': ' OR '.join(keywords),
            'language': 'en',
            'sortBy': 'publishedAt',
            'pageSize': 10,
            'apiKey': NEWSAPI_KEY
        }
        r = requests.get(url, params=params, timeout=10)
        data = r.json()
        articles.extend(data.get('articles', []))
    except Exception as e:
        print(f"NewsAPI error: {e}")

    # 2. RSS –ö–∏—Ç–∞–π
    try:
        feeds = {
            'xinhua': 'http://www.xinhuanet.com/rss/world.xml',
            'sina': 'https://rss.sina.com.cn/news/china.xml',
            'sohu': 'http://rss.news.sohu.com/rss2/news.xml'
        }
        for name, feed_url in feeds.items():
            feed = feedparser.parse(feed_url)
            for entry in feed.entries:
                title = entry.title.lower()
                if any(kw.lower() in title for kw in keywords):
                    articles.append({
                        'title': entry.title,
                        'link': entry.link,
                        'source': name,
                        'published': entry.get('published', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                    })
    except Exception as e:
        print(f"RSS error: {e}")

    return articles

# --- –°–æ–∑–¥–∞–Ω–∏–µ PDF ---
def create_pdf(articles, filename="digest.pdf"):
    from fpdf import FPDF

    pdf = FPDF()
    pdf.add_font('DejaVu', '', 'DejaVuSans.ttf', uni=True)
    pdf.add_page()
    pdf.set_font('DejaVu', size=14)
    pdf.cell(0, 10, 'üì∞ –ù–æ–≤–æ—Å—Ç–Ω–æ–π –¥–∞–π–¥–∂–µ—Å—Ç', ln=True, align='C')

    pdf.set_font('DejaVu', size=12)
    for art in articles:
        title_ru = translate_text(art['title'])
        pdf.cell(0, 8, f"‚Ä¢ {title_ru}", ln=True)
        pdf.set_text_color(0, 0, 255)
        pdf.cell(0, 8, f"  –ò—Å—Ç–æ—á–Ω–∏–∫: {art['source']}", ln=True)
        pdf.set_text_color(0, 0, 0)
        pdf.cell(0, 8, f"  –°—Å—ã–ª–∫–∞: {art['link']}", ln=True)
        pdf.ln(4)

    pdf.output(filename)
    return filename

# --- –ö–æ–º–∞–Ω–¥—ã ---
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    keyboard = [
        [InlineKeyboardButton("üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π", callback_data='manual_search')],
        [InlineKeyboardButton("üìã –ú–æ–∏ –ø–æ–¥–ø–∏—Å–∫–∏", callback_data='mysubs')],
        [InlineKeyboardButton("‚ùì –ü–æ–º–æ—â—å", callback_data='help')]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    await update.message.reply_text(
        "–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π.\n–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:",
        reply_markup=reply_markup
    )

async def button_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()

    if query.data == 'manual_search':
        await query.edit_message_text("–í–≤–µ–¥–∏—Ç–µ **–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞** –¥–ª—è –ø–æ–∏—Å–∫–∞.\n–ù–∞–ø—Ä–∏–º–µ—Ä: `–∫–æ—Å–º–æ—Å, NASA`")
        context.user_data['state'] = AWAITING_KEYWORDS

    elif query.data == 'mysubs':
        await query.edit_message_text("–£ –≤–∞—Å –ø–æ–∫–∞ –Ω–µ—Ç –ø–æ–¥–ø–∏—Å–æ–∫.")

    elif query.data == 'help':
        await query.edit_message_text("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–Ω–æ–ø–∫–∏ –Ω–∏–∂–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π.")

async def handle_keywords(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if context.user_data.get('state') != AWAITING_KEYWORDS:
        return

    keywords_input = update.message.text.strip()
    if not keywords_input:
        await update.message.reply_text("–í–≤–µ–¥–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–ª–æ–≤–æ.")
        return

    keywords = [kw.strip().lower() for kw in keywords_input.split(',') if kw.strip()]
    await update.message.reply_text(f"üîç –ò—â—É –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ: *{', '.join(keywords)}*...", parse_mode='Markdown')

    # –ü–æ–∏—Å–∫
    raw_articles = search_news(keywords)
    if not raw_articles:
        await update.message.reply_text("‚ùå –ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        context.user_data['state'] = None
        return

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 10 —Å—Ç–∞—Ç–µ–π
    articles = raw_articles[:10]

    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º
    for art in articles[:5]:
        title_ru = translate_text(art['title'])
        msg = f"üìå *{title_ru}*\nüåê {art['source']}\nüîó {art['link']}"
        await update.message.reply_text(msg, parse_mode='Markdown', disable_web_page_preview=False)

    # –°–æ–∑–¥–∞—ë–º PDF
    try:
        pdf_path = create_pdf(articles, "digest.pdf")
        await update.message.reply_document(document=open(pdf_path, 'rb'), caption="üìÑ –í–æ—Ç –≤–∞—à PDF-–¥–∞–π–¥–∂–µ—Å—Ç!")
        os.remove(pdf_path)  # —É–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ –æ—Ç–ø—Ä–∞–≤–∫–∏
    except Exception as e:
        await update.message.reply_text(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF: {e}")

    context.user_data['state'] = None

# --- –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è GitHub Actions ---
def main_github():
    print("–ó–∞–ø—É—Å–∫ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é...")
    seen = load_cache()
    keywords = ['—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏']  # –º–æ–∂–Ω–æ –±—Ä–∞—Ç—å –∏–∑ –ë–î
    articles = search_news(keywords)
    new_articles = [a for a in articles if a['link'] not in seen]

    for art in new_articles[:3]:
        title_ru = translate_text(art['title'])
        print(f"–ù–æ–≤–æ—Å—Ç—å: {title_ru} ‚Üí {art['link']}")

    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–µ—à
    for a in new_articles:
        seen.add(a['link'])
    save_cache(seen)

if __name__ == "__main__":
    # –†–µ–∂–∏–º –¥–ª—è GitHub Actions
    if os.getenv("GITHUB_ACTIONS"):
        main_github()
    else:
        # –†–µ–∂–∏–º –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
        pass