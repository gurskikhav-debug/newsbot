import os
import json
import requests
from datetime import datetime, timedelta
from deep_translator import GoogleTranslator
import feedparser
from bs4 import BeautifulSoup

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
TOKEN = os.getenv("TOKEN")
NEWSAPI_KEY = os.getenv("NEWSAPI_KEY")
ADMIN_ID = os.getenv("ADMIN_ID")
KEYWORDS_INPUT = os.getenv("KEYWORDS", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è, AI")  # –ò–∑ workflow

# --- –ö–µ—à ---
CACHE_FILE = "cache/news_cache.json"

def load_cache():
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                return set(data) if isinstance(data, list) else set()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–µ—à–∞: {e}")
    return set()

def save_cache(cache_set):
    os.makedirs("cache", exist_ok=True)
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(list(cache_set), f, ensure_ascii=False, indent=2)

# --- –ü–µ—Ä–µ–≤–æ–¥ ---
def translate_text(text):
    try:
        return GoogleTranslator(source='auto', target='ru').translate(text)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
        return text

# --- –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ---
TECH_INDICATORS = [
    '—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏', '—Å–≤–æ–π—Å—Ç–≤–∞', '–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ', '–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è', '–∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ',
    '–Ω–æ–≤–∏–Ω–∫–∞', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '—Ñ–∏–∑–∏—á–µ—Å–∫–æ–µ —è–≤–ª–µ–Ω–∏–µ', '—ç—Ñ—Ñ–µ–∫—Ç', '–º–µ—Ö–∞–Ω–∏–∑–º', '—Ä–∞–±–æ—Ç–∞', '–ø—Ä–∏–Ω—Ü–∏–ø',
    '—É–ª—É—á—à–µ–Ω–∏–µ', '—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å', '—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å', '–∏–Ω–Ω–æ–≤–∞—Ü–∏—è',
    '–≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä—ã–Ω–æ–∫', '—Ä—ã–Ω–æ—á–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª', '—ç–∫–æ–Ω–æ–º–∏—è', '–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è', '—Ä–æ–±–æ—Ç',
    'material properties', 'technical specifications', 'application in engineering',
    'physical phenomenon', 'innovation', 'market impact', 'improvement', 'efficiency'
]

# --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã ---
def extract_text_from_url(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        }
        r = requests.get(url, timeout=15, headers=headers)
        r.raise_for_status()
        soup = BeautifulSoup(r.content, 'html.parser')
        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            element.decompose()
        text = soup.get_text(separator=' ', strip=True)
        return text.lower()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}")
        return ""

# --- –ü—Ä–æ–≤–µ—Ä–∫–∞, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å—Ç–∞—Ç—å—è –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ---
def is_technical_article(text):
    return any(indicator in text for indicator in [w.lower() for w in TECH_INDICATORS])

# --- –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ 7 –¥–Ω–µ–π ---
def search_news(keywords):
    articles = []

    # 1. NewsAPI ‚Äî –∑–∞ 7 –¥–Ω–µ–π
    if NEWSAPI_KEY and keywords:
        from_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
        try:
            url = "https://newsapi.org/v2/everything"
            params = {
                'q': ' OR '.join(keywords),
                'from': from_date,
                'sortBy': 'publishedAt',
                'pageSize': 50,
                'apiKey': NEWSAPI_KEY
            }
            r = requests.get(url, params=params, timeout=15)
            if r.status_code == 200:
                data = r.json()
                for item in data.get('articles', []):
                    articles.append({
                        'title': item['title'],
                        'url': item['url'],
                        'source': item['source']['name'],
                        'published': item.get('publishedAt', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                    })
            else:
                print(f"NewsAPI error {r.status_code}: {r.text}")
        except Exception as e:
            print(f"NewsAPI –æ—à–∏–±–∫–∞: {e}")

    # 2. RSS ‚Äî –Ω–∞—É—á–Ω—ã–µ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    try:
        feeds = {
            'habr': 'https://habr.com/ru/rss/technology/',
            'nplus1': 'https://nplus1.ru/rss',
            'engineering': 'https://www.engineering.com/rss',
            'techcrunch': 'https://techcrunch.com/feed/',
            'wired': 'https://www.wired.com/feed/rss',
            'xinhua': 'http://www.xinhuanet.com/rss/world.xml',
            'sina': 'https://rss.sina.com.cn/news/china.xml'
        }
        for name, feed_url in feeds.items():
            try:
                feed = feedparser.parse(feed_url)
                for entry in feed.entries:
                    title = entry.title.lower()
                    if any(kw.lower() in title for kw in keywords):
                        articles.append({
                            'title': entry.title,
                            'url': entry.link,
                            'source': name,
                            'published': entry.get('published', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                        })
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ RSS {name}: {e}")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS: {e}")

    # 3. YouTube (—á–µ—Ä–µ–∑ RSS)
    try:
        yt_rss = f"https://www.youtube.com/feeds/videos.xml?user=TechInsider"
        feed = feedparser.parse(yt_rss)
        for entry in feed.entries:
            title = entry.title.lower()
            if any(kw.lower() in title for kw in keywords):
                desc = entry.description.lower()
                if is_technical_article(desc):
                    articles.append({
                        'title': entry.title,
                        'url': entry.link,
                        'source': 'YouTube',
                        'published': entry.get('published', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                    })
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ YouTube: {e}")

    # 4. –î–∑–µ–Ω, Telegram, Instagram ‚Äî —á–µ—Ä–µ–∑ RSS –∏–ª–∏ API (—É–ø—Ä–æ—â—ë–Ω–Ω–æ)
    try:
        zen_feeds = [
            'https://zen.yandex.ru/rss/technologies',
            'https://zen.yandex.ru/rss/science'
        ]
        for feed_url in zen_feeds:
            try:
                feed = feedparser.parse(feed_url)
                for entry in feed.entries:
                    title = entry.title.lower()
                    if any(kw.lower() in title for kw in keywords):
                        articles.append({
                            'title': entry.title,
                            'url': entry.link,
                            'source': '–î–∑–µ–Ω',
                            'published': entry.get('published', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                        })
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –î–∑–µ–Ω: {e}")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –î–∑–µ–Ω: {e}")

    return articles

# --- –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Telegram ---
def send_message(chat_id, text, parse_mode='HTML', disable_preview=False):
    if not chat_id:
        print("‚ùå chat_id –Ω–µ –∑–∞–¥–∞–Ω")
        return
    try:
        # –≠–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è HTML
        text = text.replace('<', '<').replace('>', '>')
        url = f"https://api.telegram.org/bot{TOKEN}/sendMessage"
        data = {
            "chat_id": chat_id,
            "text": text,
            "parse_mode": parse_mode,
            "disable_web_page_preview": not disable_preview
        }
        response = requests.post(url, data=data, timeout=10)
        if response.status_code == 200:
            print(f"‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ {chat_id}")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {response.status_code}, {response.text}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ: {e}")

# --- –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ---
def main():
    print("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω (—Ä–µ–∂–∏–º GitHub Actions)")

    # –ü–∞—Ä—Å–∏–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    keywords = [kw.strip() for kw in KEYWORDS_INPUT.split(',') if kw.strip()]
    print(f"üîç –ü–æ–∏—Å–∫ –ø–æ: {keywords}")

    if not keywords:
        print("‚ùå –ù–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞")
        if ADMIN_ID:
            send_message(ADMIN_ID, "‚ùå –ù–µ —É–∫–∞–∑–∞–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞.")
        return

    seen_urls = load_cache()
    raw_articles = search_news(keywords)
    print(f"–ü–æ–ª—É—á–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(raw_articles)}")

    if not raw_articles:
        print("‚ùå –ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        if ADMIN_ID:
            send_message(ADMIN_ID, "‚ùå –ù–æ–≤–æ—Å—Ç–∏ –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º —Å–ª–æ–≤–∞–º –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return

    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
    filtered_articles = []
    for art in raw_articles:
        text = extract_text_from_url(art['url'])
        if any(kw.lower() in text for kw in keywords) and is_technical_article(text):
            filtered_articles.append(art)

    print(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º: {len(filtered_articles)}")

    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
    articles = [a for a in filtered_articles if a.get('url') not in seen_urls]

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 20 –Ω–æ–≤–æ—Å—Ç—è–º–∏
    selected = articles[:20]
    print(f"–û—Ç–ø—Ä–∞–≤–ª—è–µ–º: {len(selected)} –Ω–æ–≤–æ—Å—Ç–µ–π")

    if not selected:
        print("–ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.")
        if ADMIN_ID:
            send_message(ADMIN_ID, "üì≠ –ù–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –≤–∞—à–∏–º —Å–ª–æ–≤–∞–º.")
        return

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
    msg = f"<b>üîß –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –Ω–µ–¥–µ–ª—é –ø–æ –∑–∞–ø—Ä–æ—Å—É:</b> <code>{', '.join(keywords)}</code>\n\n"
    for art in selected:
        title_ru = translate_text(art['title'])
        source = art.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        msg += f"üìå <b>{title_ru}</b>\n"
        msg += f"üåê {source}\n"
        msg += f"üîó <a href='{art['url']}'>–°—Å—ã–ª–∫–∞</a>\n\n"

    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º
    if ADMIN_ID:
        send_message(ADMIN_ID, msg, disable_preview=False)

    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–µ—à
    for art in selected:
        url = art.get('url')
        if url:
            seen_urls.add(url)
    save_cache(seen_urls)

    print("‚úÖ –†–∞—Å—Å—ã–ª–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

# --- –ó–∞–ø—É—Å–∫ ---
if __name__ == "__main__":
    main()
