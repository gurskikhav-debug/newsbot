import os
import json
import requests
from datetime import datetime
from deep_translator import GoogleTranslator
import feedparser
from fpdf import FPDF
from telegram import Update
from telegram.ext import Application, CommandHandler, CallbackQueryHandler, MessageHandler, filters, ContextTypes

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
TOKEN = os.getenv("TOKEN")
NEWSAPI_KEY = os.getenv("NEWSAPI_KEY")
ADMIN_ID = os.getenv("ADMIN_ID")

# --- –°–æ—Å—Ç–æ—è–Ω–∏—è ---
AWAITING_KEYWORDS = "awaiting_keywords"

# --- –ö–µ—à ---
CACHE_FILE = "cache/news_cache.json"

def load_cache():
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                return set(data) if isinstance(data, list) else set()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–µ—à–∞: {e}")
    return set()

def save_cache(cache_set):
    os.makedirs("cache", exist_ok=True)
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(list(cache_set), f, ensure_ascii=False, indent=2)

# --- –ü–µ—Ä–µ–≤–æ–¥ ---
def translate_text(text):
    try:
        return GoogleTranslator(source='auto', target='ru').translate(text)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
        return text

# --- –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π ---
def search_news(keywords):
    articles = []

    # 1. NewsAPI
    if NEWSAPI_KEY:
        try:
            url = "https://newsapi.org/v2/everything"
            params = {
                'q': ' OR '.join(keywords),
                'language': 'en',
                'sortBy': 'publishedAt',
                'pageSize': 10,
                'apiKey': NEWSAPI_KEY
            }
            r = requests.get(url, params=params, timeout=10)
            if r.status_code == 200:
                data = r.json()
                articles.extend(data.get('articles', []))
            else:
                print(f"NewsAPI error {r.status_code}: {r.text}")
        except Exception as e:
            print(f"NewsAPI –æ—à–∏–±–∫–∞: {e}")

    # 2. RSS –∏–∑ –ö–∏—Ç–∞—è
    try:
        feeds = {
            'xinhua': 'http://www.xinhuanet.com/rss/world.xml',
            'sina': 'https://rss.sina.com.cn/news/china.xml',
            'sohu': 'http://rss.news.sohu.com/rss2/news.xml'
        }
        for name, feed_url in feeds.items():
            try:
                feed = feedparser.parse(feed_url)
                if not feed.entries:
                    print(f"‚ö†Ô∏è RSS {name} –ø—É—Å—Ç–æ–π")
                else:
                    print(f"‚úÖ {name}: {len(feed.entries)} —Å—Ç–∞—Ç–µ–π")
                for entry in feed.entries:
                    title = entry.title.lower()
                    if any(kw.lower() in title for kw in keywords):
                        articles.append({
                            'title': entry.title,
                            'link': entry.link,
                            'source': name,
                            'published': entry.get('published', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                        })
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ RSS {name}: {e}")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS: {e}")

    return articles

# --- –°–æ–∑–¥–∞–Ω–∏–µ PDF ---
def create_pdf(articles, filename="digest.pdf"):
    try:
        font_path = "DejaVuSans.ttf"
        if not os.path.exists(font_path):
            print("–®—Ä–∏—Ñ—Ç DejaVuSans.ttf –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return None

        pdf = FPDF()
        pdf.add_font('DejaVu', '', font_path, uni=True)
        pdf.add_page()
        pdf.set_font('DejaVu', size=14)
        pdf.cell(0, 10, 'üì∞ –ù–æ–≤–æ—Å—Ç–Ω–æ–π –¥–∞–π–¥–∂–µ—Å—Ç', ln=True, align='C')

        pdf.set_font('DejaVu', size=12)
        for art in articles:
            title_ru = translate_text(art['title'])
            pdf.cell(0, 8, f"‚Ä¢ {title_ru}", ln=True)
            pdf.set_text_color(0, 0, 255)
            pdf.cell(0, 8, f"  –ò—Å—Ç–æ—á–Ω–∏–∫: {art['source']}", ln=True)
            pdf.set_text_color(0, 0, 0)
            pdf.cell(0, 8, f"  –°—Å—ã–ª–∫–∞: {art['link']}", ln=True)
            pdf.ln(4)

        pdf.output(filename)
        return filename
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è PDF: {e}")
        return None

# --- –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Telegram ---
async def send_message(chat_id, text, parse_mode='Markdown', disable_preview=False):
    url = f"https://api.telegram.org/bot{TOKEN}/sendMessage"
    data = {
        "chat_id": chat_id,
        "text": text,
        "parse_mode": parse_mode,
        "disable_web_page_preview": disable_preview
    }
    try:
        requests.post(url, data=data, timeout=10)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")

# --- –û—Ç–ø—Ä–∞–≤–∫–∞ –æ—à–∏–±–æ–∫ –∞–¥–º–∏–Ω—É ---
async def send_error(msg):
    if ADMIN_ID and TOKEN:
        await send_message(ADMIN_ID, f"‚ùå –û—à–∏–±–∫–∞ –±–æ—Ç–∞:\n\n`{msg}`")

# --- –ö–æ–º–∞–Ω–¥—ã ---
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    keyboard = [
        [InlineKeyboardButton("üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π", callback_data='manual_search')],
        [InlineKeyboardButton("üìã –ú–æ–∏ –ø–æ–¥–ø–∏—Å–∫–∏", callback_data='mysubs')],
        [InlineKeyboardButton("‚ùì –ü–æ–º–æ—â—å", callback_data='help')]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    await update.message.reply_text(
        "–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π.\n–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:",
        reply_markup=reply_markup
    )

async def button_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()

    if query.data == 'manual_search':
        await query.edit_message_text("–í–≤–µ–¥–∏—Ç–µ **–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞** –¥–ª—è –ø–æ–∏—Å–∫–∞.\n–ù–∞–ø—Ä–∏–º–µ—Ä: `–∫–æ—Å–º–æ—Å, NASA`")
        context.user_data['state'] = AWAITING_KEYWORDS

    elif query.data == 'mysubs':
        await query.edit_message_text("–£ –≤–∞—Å –ø–æ–∫–∞ –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–¥–ø–∏—Å–æ–∫.")

    elif query.data == 'help':
        await query.edit_message_text(
            "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ:\n"
            "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π ‚Äî –∏—â–∏—Ç–µ –ø–æ —Ç–µ–º–µ\n"
            "üìã –ú–æ–∏ –ø–æ–¥–ø–∏—Å–∫–∏ ‚Äî —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—ã–ª–∫–æ–π\n"
            "–í—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å –ø–µ—Ä–µ–≤–æ–¥–æ–º –Ω–∞ —Ä—É—Å—Å–∫–∏–π."
        )

async def handle_keywords(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if context.user_data.get('state') != AWAITING_KEYWORDS:
        return

    keywords_input = update.message.text.strip()
    if not keywords_input:
        await update.message.reply_text("–í–≤–µ–¥–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–ª–æ–≤–æ.")
        return

    keywords = [kw.strip().lower() for kw in keywords_input.split(',') if kw.strip()]
    await update.message.reply_text(f"üîç –ò—â—É –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ: *{', '.join(keywords)}*...", parse_mode='Markdown')

    # –ü–æ–∏—Å–∫
    raw_articles = search_news(keywords)
    print(f"–ü–æ–ª—É—á–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(raw_articles)}")

    if not raw_articles:
        await update.message.reply_text("‚ùå –ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        context.user_data['state'] = None
        return

    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
    seen_urls = load_cache()
    articles = [a for a in raw_articles if a.get('link') not in seen_urls]

    if not articles:
        await update.message.reply_text("–ù–æ–≤–æ—Å—Ç–∏ —É–∂–µ –±—ã–ª–∏ –ø–æ–∫–∞–∑–∞–Ω—ã —Ä–∞–Ω–µ–µ.")
        context.user_data['state'] = None
        return

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
    for art in articles[:5]:
        title_ru = translate_text(art['title'])
        source = art.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        msg = f"üìå *{title_ru}*\nüåê {source}\nüîó {art['link']}"
        await update.message.reply_text(msg, parse_mode='Markdown', disable_web_page_preview=False)

    # –°–æ–∑–¥–∞—ë–º PDF
    pdf_path = create_pdf(articles, "digest.pdf")
    if pdf_path and os.path.exists(pdf_path):
        await update.message.reply_document(
            document=open(pdf_path, 'rb'),
            caption="üìÑ PDF-–¥–∞–π–¥–∂–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π"
        )
        os.remove(pdf_path)
    else:
        await update.message.reply_text("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å PDF.")

    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–µ—à
    for art in articles:
        url = art.get('link')
        if url:
            seen_urls.add(url)
    save_cache(seen_urls)

    context.user_data['state'] = None

# --- –ó–∞–ø—É—Å–∫ –≤ GitHub Actions ---
def main():
    print("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω (—Ä–µ–∂–∏–º GitHub Actions)")
    try:
        seen_urls = load_cache()
        keywords = ['—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '–∫–æ—Å–º–æ—Å', 'AI', 'ÁßëÊäÄ', 'technology']
        print(f"üîç –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords}")
        raw_articles = search_news(keywords)
        print(f"–ü–æ–ª—É—á–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(raw_articles)}")

        if not raw_articles:
            print("‚ùå –ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã ‚Äî –≤–æ–∑–º–æ–∂–Ω–æ, –æ—à–∏–±–∫–∞ –≤ API –∏–ª–∏ RSS")
            return

        new_articles = [a for a in raw_articles if a.get('link') not in seen_urls]
        print(f"‚úÖ –ù–æ–≤—ã—Ö: {len(new_articles)}")

        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–µ—à
        for art in new_articles:
            url = art.get('link')
            if url:
                seen_urls.add(url)
        save_cache(seen_urls)

        print("‚úÖ –ö–µ—à –æ–±–Ω–æ–≤–ª—ë–Ω.")

    except Exception as e:
        error_msg = f"{type(e).__name__}: {str(e)[:500]}"
        print(f"üî¥ –û—à–∏–±–∫–∞: {error_msg}")
        if ADMIN_ID and TOKEN:
            requests.post(
                f"https://api.telegram.org/bot{TOKEN}/sendMessage",
                data={"chat_id": ADMIN_ID, "text": f"‚ùå –û—à–∏–±–∫–∞: `{error_msg}`"}
            )

# --- –ó–∞–ø—É—Å–∫ ---
if __name__ == "__main__":
    if os.getenv("GITHUB_ACTIONS"):
        main()
    else:
        print("–ó–∞–ø—É—Å–∫ –≤ —Ä–µ–∂–∏–º–µ polling –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é")